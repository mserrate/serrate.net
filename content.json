{"meta":{"title":"MarÃ§al Serrate","subtitle":null,"description":null,"author":"MarÃ§al Serrate","url":"http://www.serrate.net"},"pages":[{"title":"About","date":"2017-07-02T16:57:03.000Z","updated":"2017-07-02T16:57:03.000Z","comments":false,"path":"about/index.html","permalink":"http://www.serrate.net/about/index.html","excerpt":"","text":"My name is MarÃ§al Serrate and Iâ€™m a technical architect specialized in enterprise solutions and software design. I have been working on national and international projects applying SOA, EDA, DDD, CQRS and NoSQL on the .NET &amp; JVM platforms. I also enjoy working on BigData technologies such as Storm, Kafka, Hadoop &amp; Cassandra. Iâ€™ve been performing the Udi Dahanâ€™s Enterprise Development with NServiceBus workshop several times in the past."},{"title":"Events","date":"2017-07-02T16:57:55.000Z","updated":"2017-07-02T16:57:55.000Z","comments":false,"path":"events/index.html","permalink":"http://www.serrate.net/events/index.html","excerpt":"","text":"Here is where I will be speaking along with my speaking history and other event participation. Upcoming EventsPast Events Actor model with Akka.netCatDotNet | May, 2015 Complex Event Processing with Event StoreMicrosoft DotNetSpain Conference | Feb, 2015 NoSQL with RavenDBBarcelona Developers Conference | Dec, 2012 DDD, SOA, ESB&#8230; Help!Barcelona Developers Conference | Dec, 2012 SOA, DDD &amp; CQRS with NServiceBus (Part 2)Webcast, SecondNug | Nov, 2012 Introduction to SOA with NServiceBus (Part 1)Webcast, SecondNug | Sep, 2012 Distributed Systems with NServiceBusBarcelona, IASA | May, 2012 Distributed Systems with NServiceBusMicrosoft Madrid, IASA | Apr, 2012 Command and Query Responsibility Segregation (CQRS)Webcast, SecondNug | May, 2012 Practical ASP.NET MVC 3Barcelona, CatDotNet | Feb, 2011 First Steps with CSLA.NETWebcast, SecondNug | May, 2008"}],"posts":[{"title":"Preview of Kafka Streams","slug":"preview-of-kafka-streams","date":"2016-03-15T01:02:29.000Z","updated":"2017-07-02T16:41:26.000Z","comments":true,"path":"2016/03/15/preview-of-kafka-streams/","link":"","permalink":"http://www.serrate.net/2016/03/15/preview-of-kafka-streams/","excerpt":"","text":"The preview of Kafka Streams, which is one of the main features of the upcoming Apache Kafka 0.10, was announced by Jay Kreps this week. Kafka joins the Stream Processing clubKafka Streams is a library to build streaming applications using Kafka topics as input/output. Kafka Streams is in the same league as other streaming systems such as: Apache Storm, Apache Flink and, not surprisingly, Apache Samza which also uses Kafka for input or output of streams. One of the main advantages is that if you&#8217;re already using Apache Kafka and you need real-time processing, you just need to use the library and you are good to go. Other important features are: stateful processing, windowing and ability to be deployed using your preferred solution: a simple command line, Mesos, YARN or kubernetes and docker if you&#8217;re a container party boy. Streams and TablesOne of the key concepts in Kafka Streams is the support of KStream and KTable. That isn&#8217;t a new concept if you come from the Event Sourcing world: the KStream is the append-only event store where its state is given by replaying the events from the beginning of time until the last event whereas KTable is the snapshot or projection of the current state of the stream given a point in time. Example: Twitter Hashtags Job Show me the code!You can find the complete example here: https://github.com/mserrate/kafka-streams-app 123456789KStream&lt;String, JsonNode&gt; source = builder.stream(stringDeserializer, jsonDeserializer, \"streams-hashtag-input\");KTable&lt;String, Long&gt; counts = source .filter(new HashtagFilter()) .flatMapValues(new HashtagSplitter()) .map(new HashtagMapper()) .countByKey(stringSerializer, longSerializer, stringDeserializer, longDeserializer, \"Counts\");counts.to(\"streams-hashtag-count-output\", stringSerializer, longSerializer); For this example I&#8217;ve been using a simple TweetProducer who connects to the Twitter Streaming API and sends JSON events to a Kafka topic. This topic is read as a KStream and then we begin the process: Filter out the tweets without hashtags Apply a flatMapValues (we are just interested in the values, not the keys) to split the different hashtags in a tweet Apply a map to return a key (hashtag) value (hashtag) as we want to aggregate by hashtag Aggregate the streams per key (the hashtag) and count them &nbsp; Finally we send the KTable to the output queue.","categories":[{"name":"Architecture","slug":"Architecture","permalink":"http://www.serrate.net/categories/Architecture/"}],"tags":[{"name":"BigData","slug":"BigData","permalink":"http://www.serrate.net/tags/BigData/"},{"name":"Kafka","slug":"Kafka","permalink":"http://www.serrate.net/tags/Kafka/"},{"name":"Storm","slug":"Storm","permalink":"http://www.serrate.net/tags/Storm/"}]},{"title":"Sentiment analysis of tweets","slug":"sentiment-analysis-of-tweets","date":"2016-02-07T15:40:34.000Z","updated":"2017-07-02T16:42:06.000Z","comments":true,"path":"2016/02/07/sentiment-analysis-of-tweets/","link":"","permalink":"http://www.serrate.net/2016/02/07/sentiment-analysis-of-tweets/","excerpt":"In the previous post I have presented an overview of the topology used to analyse twitter streams with Kafka and Storm. Now it&#8217;s time to cover the technical details of the twitter topology. Twitter Topology The declaration of the storm topology using KafkaSpout to read the tweets from a kafka queue: 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576public class TwitterProcessorTopology extends BaseTopology &#123; public TwitterProcessorTopology(String configFileLocation) throws Exception &#123; super(configFileLocation); &#125; private void configureKafkaSpout(TopologyBuilder topology) &#123; BrokerHosts hosts = new ZkHosts(topologyConfig.getProperty(\"zookeeper.host\")); SpoutConfig spoutConfig = new SpoutConfig( hosts, topologyConfig.getProperty(\"kafka.twitter.raw.topic\"), topologyConfig.getProperty(\"kafka.zkRoot\"), topologyConfig.getProperty(\"kafka.consumer.group\")); spoutConfig.scheme= new SchemeAsMultiScheme(new StringScheme()); KafkaSpout kafkaSpout= new KafkaSpout(spoutConfig); topology.setSpout(\"twitterSpout\", kafkaSpout); &#125; private void configureBolts(TopologyBuilder topology) &#123; // filtering topology.setBolt(\"twitterFilter\", new TwitterFilterBolt(), 4) .shuffleGrouping(\"twitterSpout\"); // sanitization topology.setBolt(\"textSanitization\", new TextSanitizationBolt(), 4) .shuffleGrouping(\"twitterFilter\"); // sentiment analysis topology.setBolt(\"sentimentAnalysis\", new SentimentAnalysisBolt(), 4) .shuffleGrouping(\"textSanitization\"); // persist tweets with analysis to Cassandra topology.setBolt(\"sentimentAnalysisToCassandra\", new SentimentAnalysisToCassandraBolt(topologyConfig), 4) .shuffleGrouping(\"sentimentAnalysis\"); // divide sentiment by hashtag topology.setBolt(\"hashtagSplitter\", new HashtagSplitterBolt(), 4) .shuffleGrouping(\"textSanitization\"); // persist hashtags to Cassandra topology.setBolt(\"hashtagCounter\", new HashtagCounterBolt(), 4) .fieldsGrouping(\"hashtagSplitter\", new Fields(\"tweet_hashtag\")); topology.setBolt(\"topHashtag\", new TopHashtagBolt()) .globalGrouping(\"hashtagCounter\"); topology.setBolt(\"topHashtagToCassandra\", new TopHashtagToCassandraBolt(topologyConfig), 4) .shuffleGrouping(\"topHashtag\"); &#125; private void buildAndSubmit() throws Exception &#123; TopologyBuilder builder = new TopologyBuilder(); configureKafkaSpout(builder); configureBolts(builder); Config config = new Config(); //set producer properties Properties props = new Properties(); props.put(\"metadata.broker.list\", topologyConfig.getProperty(\"kafka.broker.list\")); props.put(\"request.required.acks\", \"1\"); props.put(\"serializer.class\", \"kafka.serializer.StringEncoder\"); config.put(KafkaBolt.KAFKA_BROKER_PROPERTIES, props); StormSubmitter.submitTopology(\"twitter-processor\", config, builder.createTopology()); &#125; public static void main(String[] args) throws Exception &#123; String configFileLocation = args[0]; TwitterProcessorTopology topology = new TwitterProcessorTopology(configFileLocation); topology.buildAndSubmit(); &#125;&#125;","text":"In the previous post I have presented an overview of the topology used to analyse twitter streams with Kafka and Storm. Now it&#8217;s time to cover the technical details of the twitter topology. Twitter Topology The declaration of the storm topology using KafkaSpout to read the tweets from a kafka queue: 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576public class TwitterProcessorTopology extends BaseTopology &#123; public TwitterProcessorTopology(String configFileLocation) throws Exception &#123; super(configFileLocation); &#125; private void configureKafkaSpout(TopologyBuilder topology) &#123; BrokerHosts hosts = new ZkHosts(topologyConfig.getProperty(\"zookeeper.host\")); SpoutConfig spoutConfig = new SpoutConfig( hosts, topologyConfig.getProperty(\"kafka.twitter.raw.topic\"), topologyConfig.getProperty(\"kafka.zkRoot\"), topologyConfig.getProperty(\"kafka.consumer.group\")); spoutConfig.scheme= new SchemeAsMultiScheme(new StringScheme()); KafkaSpout kafkaSpout= new KafkaSpout(spoutConfig); topology.setSpout(\"twitterSpout\", kafkaSpout); &#125; private void configureBolts(TopologyBuilder topology) &#123; // filtering topology.setBolt(\"twitterFilter\", new TwitterFilterBolt(), 4) .shuffleGrouping(\"twitterSpout\"); // sanitization topology.setBolt(\"textSanitization\", new TextSanitizationBolt(), 4) .shuffleGrouping(\"twitterFilter\"); // sentiment analysis topology.setBolt(\"sentimentAnalysis\", new SentimentAnalysisBolt(), 4) .shuffleGrouping(\"textSanitization\"); // persist tweets with analysis to Cassandra topology.setBolt(\"sentimentAnalysisToCassandra\", new SentimentAnalysisToCassandraBolt(topologyConfig), 4) .shuffleGrouping(\"sentimentAnalysis\"); // divide sentiment by hashtag topology.setBolt(\"hashtagSplitter\", new HashtagSplitterBolt(), 4) .shuffleGrouping(\"textSanitization\"); // persist hashtags to Cassandra topology.setBolt(\"hashtagCounter\", new HashtagCounterBolt(), 4) .fieldsGrouping(\"hashtagSplitter\", new Fields(\"tweet_hashtag\")); topology.setBolt(\"topHashtag\", new TopHashtagBolt()) .globalGrouping(\"hashtagCounter\"); topology.setBolt(\"topHashtagToCassandra\", new TopHashtagToCassandraBolt(topologyConfig), 4) .shuffleGrouping(\"topHashtag\"); &#125; private void buildAndSubmit() throws Exception &#123; TopologyBuilder builder = new TopologyBuilder(); configureKafkaSpout(builder); configureBolts(builder); Config config = new Config(); //set producer properties Properties props = new Properties(); props.put(\"metadata.broker.list\", topologyConfig.getProperty(\"kafka.broker.list\")); props.put(\"request.required.acks\", \"1\"); props.put(\"serializer.class\", \"kafka.serializer.StringEncoder\"); config.put(KafkaBolt.KAFKA_BROKER_PROPERTIES, props); StormSubmitter.submitTopology(\"twitter-processor\", config, builder.createTopology()); &#125; public static void main(String[] args) throws Exception &#123; String configFileLocation = args[0]; TwitterProcessorTopology topology = new TwitterProcessorTopology(configFileLocation); topology.buildAndSubmit(); &#125;&#125; 1. Filter BoltFirst of all, we are going to filter the tweets that we are interested in. As we are going to perform the sentiment analysis just to tweets in english, we are filtering on this property: 12345678910111213141516171819202122232425262728293031323334353637public class TwitterFilterBolt extends BaseBasicBolt &#123; private static final Logger LOG = LoggerFactory.getLogger(TwitterFilterBolt.class); @Override public void execute(Tuple tuple, BasicOutputCollector collector) &#123; try &#123; JSONObject object = (JSONObject)JSONValue.parseWithException(tuple.getString(0)); if (object.containsKey(\"lang\") &amp;amp;amp;&amp;amp;amp; \"en\".equals(object.get(\"lang\"))) &#123; long id = (long)object.get(\"id\"); String text = (String)object.get(\"text\"); String createdAt = (String)object.get(\"created_at\"); JSONObject entities= (JSONObject)object.get(\"entities\"); JSONArray hashtags =(JSONArray)entities.get(\"hashtags\"); HashSet&amp;amp;lt;String&amp;amp;gt; hashtagList = new HashSet&amp;amp;lt;String&amp;amp;gt;(); for(Object hashtag : hashtags) &#123; hashtagList.add(((String)((JSONObject)hashtag).get(\"text\")).toLowerCase()); &#125; collector.emit(new Values(id, text, hashtagList, createdAt)); &#125; else &#123; LOG.debug(\"Ignoring non-english tweets\"); &#125; &#125; catch (ParseException e) &#123; LOG.error(\"Error parsing tweet: \" + e.getMessage()); &#125; &#125; @Override public void declareOutputFields(OutputFieldsDeclarer declarer) &#123; declarer.declare(new Fields(\"tweet_id\", \"tweet_text\", \"tweet_hashtags\", \"tweet_created_at\")); &#125;&#125; 2. Sanitization BoltThen, we will sanitise our tweets by converting accented characters into unaccented characters and by removing single letters or numbers: 1234567891011121314151617181920212223public class TextSanitizationBolt extends BaseBasicBolt &#123; private static final Logger LOG = LoggerFactory.getLogger(TextSanitizationBolt.class); @Override public void execute(Tuple tuple, BasicOutputCollector collector) &#123; String text = tuple.getString(1); String normalizedText = Normalizer.normalize(text, Normalizer.Form.NFD); text = normalizedText.replaceAll(\"\\\\p&#123;InCombiningDiacriticalMarks&#125;+\", \"\"); text = text.replaceAll(\"[^\\\\p&#123;L&#125;\\\\p&#123;Nd&#125;]+\", \" \").toLowerCase(); collector.emit(new Values( tuple.getLongByField(\"tweet_id\"), text, tuple.getValueByField(\"tweet_hashtags\"), tuple.getStringByField(\"tweet_created_at\"))); &#125; @Override public void declareOutputFields(OutputFieldsDeclarer declarer) &#123; declarer.declare(new Fields(\"tweet_id\", \"tweet_text\", \"tweet_hashtags\", \"tweet_created_at\")); &#125;&#125; 3a. Sentiment Analysis BoltOn this bolt we are scoring the tweet by each of its words using SentiWordNet. That&#8217;s not the best way to do it as it can have false positives or negatives given that it does the classification word by word independently: it does not cover the tweet context or sarcasm, etc. but that&#8217;s ok for a sample ðŸ™‚ 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849public class SentimentAnalysisBolt extends BaseBasicBolt &#123; private static final Logger LOG = LoggerFactory.getLogger(SentimentAnalysisBolt.class); SentiWordNet sentiWordNet; @Override public void prepare(Map stormConf, TopologyContext context) &#123; try &#123; sentiWordNet = SentiWordNet.getInstance(); &#125; catch (IOException e) &#123; LOG.error(\"Problem parsing SentiWordNet file: \" + e.getMessage()); &#125; &#125; @Override public void execute(Tuple tuple, BasicOutputCollector collector) &#123; double count = 0; String text = tuple.getStringByField(\"tweet_text\"); try &#123; String delimiters = \"\\\\W\"; String[] tokens = text.split(delimiters); double feeling = 0; for (int i = 0; i &amp;amp;lt; tokens.length; ++i) &#123; if (!tokens[i].isEmpty()) &#123; // Search as adjective feeling = sentiWordNet.extract(tokens[i], \"a\"); count += feeling; &#125; &#125; LOG.info(\"text: \" + text + \" count: \" + count); &#125; catch (Exception e) &#123; LOG.error(\"Problem found when classifying the text: \" + e.getMessage()); &#125; collector.emit(new Values( tuple.getLongByField(\"tweet_id\"), text, count, tuple.getValueByField(\"tweet_hashtags\"), tuple.getStringByField(\"tweet_created_at\"))); &#125; @Override public void declareOutputFields(OutputFieldsDeclarer declarer) &#123; declarer.declare(new Fields(\"tweet_id\", \"tweet_text\", \"tweet_sentiment\", \"tweet_hashtags\", \"tweet_created_at\")); &#125;&#125; 4a. Save results to CassandraFinally, we will store to Cassandra the the tweet with the score and its hashtags if any: 12345678910111213141516171819202122232425262728public class SentimentAnalysisToCassandraBolt extends CassandraBaseBolt &#123; private static final Logger LOG = LoggerFactory.getLogger(SentimentAnalysisToCassandraBolt.class); public SentimentAnalysisToCassandraBolt(Properties properties) &#123; super(properties); &#125; @Override public void execute(Tuple tuple, BasicOutputCollector collector) &#123; HashSet&lt;String&gt; hashtags = (HashSet&lt;String&gt;)tuple.getValueByField(\"tweet_hashtags\"); Statement statement = QueryBuilder.update(\"tweet_sentiment_analysis\") .with(QueryBuilder.set(\"tweet\", tuple.getStringByField(\"tweet_text\"))) .and(QueryBuilder.set(\"sentiment\", tuple.getDoubleByField(\"tweet_sentiment\"))) .and(QueryBuilder.addAll(\"hashtags\", hashtags)) .and(QueryBuilder.set(\"created_at\", tuple.getStringByField(\"tweet_created_at\"))) .where(QueryBuilder.eq(\"tweet_id\", tuple.getLongByField(\"tweet_id\"))); LOG.debug(statement.toString()); session.execute(statement); &#125; @Override public void declareOutputFields(OutputFieldsDeclarer declarer) &#123; &#125;&#125; 3b. Hashtag Splitter BoltThat branch of the topology graph is responsible for splitting the different hashtags and emitting a tuple per hashtag to the next bolt. That&#8217;s why we inherit from BaseRichBolt in order to manually ACK the tuple after all hashtags have been emitted. 12345678910111213141516171819202122232425public class HashtagSplitterBolt extends BaseRichBolt &#123; OutputCollector collector; Map&lt;String, Integer&gt; count = new HashMap&lt;String, Integer&gt;(); @Override public void prepare(Map map, TopologyContext topologyContext, OutputCollector collector) &#123; this.collector = collector; &#125; @Override public void execute(Tuple tuple) &#123; HashSet&lt;String&gt; hashtags = (HashSet&lt;String&gt;)tuple.getValueByField(\"tweet_hashtags\"); for (String hashtag : hashtags) &#123; collector.emit(new Values(hashtag)); &#125; collector.ack(tuple); &#125; @Override public void declareOutputFields(OutputFieldsDeclarer declarer) &#123; declarer.declare(new Fields(\"tweet_hashtag\")); &#125;&#125; 4b. Hashtag Counter BoltIn this particular case, we are using Fields Grouping because we want to partition the stream by hashtag. It means that the same hashtag will always go to the same task. Thus, we can use a hashmap to count the number of occurrences of a hashtag: 1234567891011121314151617181920212223public class HashtagCounterBolt extends BaseBasicBolt &#123; private static final Logger LOG = LoggerFactory.getLogger(HashtagCounterBolt.class); private Map&lt;String, Long&gt; hashtag_count = new HashMap&lt;String, Long&gt;(); @Override public void execute(Tuple tuple, BasicOutputCollector collector) &#123; String hashtag = tuple.getStringByField(\"tweet_hashtag\"); Long count = hashtag_count.get(hashtag); if (count == null) count = 0L; count++; hashtag_count.put(hashtag, count); collector.emit(new Values(hashtag, count)); &#125; @Override public void declareOutputFields(OutputFieldsDeclarer declarer) &#123; declarer.declare(new Fields(\"hashtag\", \"count\")); &#125;&#125; 5b. Top Hashtag BoltFor that bolt, we are using Global Grouping to get the top 20 hashtags. Global grouping means that all hashtags will go to the same bolt&#8217;s task. For that use case, we need a sliding windows in order to get the top 20 hashtags every 10 seconds. We are relying on the Storm Tick Tuple feature. For normal tuples we just do the ranking of hashtags and, when a tick tuple is received (configured to get it every 10sec) we emit the ranking calculated over this window of time. 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960public class TopHashtagBolt extends BaseBasicBolt &#123; List&lt;List&gt; rankings = new ArrayList&lt;List&gt;(); private static final Logger LOG = LoggerFactory.getLogger(TopHashtagBolt.class); private static final Integer TOPN = 20; private static final Integer TICK_FREQUENCY = 10; @Override public void execute(Tuple tuple, BasicOutputCollector collector) &#123; if (isTickTuple(tuple)) &#123; LOG.debug(\"Tick: \" + rankings); collector.emit(new Values(new ArrayList(rankings))); &#125; else &#123; rankHashtag(tuple); &#125; &#125; @Override public void declareOutputFields(OutputFieldsDeclarer declarer) &#123; declarer.declare(new Fields(\"tophashtags\")); &#125; @Override public Map&lt;String, Object&gt; getComponentConfiguration() &#123; Config conf = new Config(); conf.put(Config.TOPOLOGY_TICK_TUPLE_FREQ_SECS, TICK_FREQUENCY); return conf; &#125; private void rankHashtag(Tuple tuple) &#123; String hashtag = tuple.getStringByField(\"hashtag\"); Integer existingIndex = find(hashtag); if (null != existingIndex) rankings.set(existingIndex, tuple.getValues()); else rankings.add(tuple.getValues()); Collections.sort(rankings, new Comparator&lt;List&gt;() &#123; @Override public int compare(List o1, List o2) &#123; return compareRanking(o1, o2); &#125; &#125;); shrinkRanking(); &#125; private Integer find(String hashtag) &#123; for(int i = 0; i &lt; rankings.size(); ++i) &#123; String current = (String) rankings.get(i).get(0); if (current.equals(hashtag)) &#123; return i; &#125; &#125; return null; &#125; private int compareRanking(List one, List two) &#123; long valueOne = (Long) one.get(1); long valueTwo = (Long) two.get(1); long delta = valueTwo - valueOne; if(delta &gt; 0) &#123; return 1; &#125; else if (delta &lt; 0) &#123; return -1; &#125; else &#123; return 0; &#125; &#125; private void shrinkRanking() &#123; int size = rankings.size(); if (TOPN &gt;= size) return; for (int i = TOPN; i &lt; size; i++) &#123; rankings.remove(rankings.size() - 1); &#125; &#125; private static boolean isTickTuple(Tuple tuple) &#123; return tuple.getSourceComponent().equals(Constants.SYSTEM_COMPONENT_ID) &amp;amp;&amp;amp; tuple.getSourceStreamId().equals(Constants.SYSTEM_TICK_STREAM_ID); &#125;&#125; 6b. Save results to CassandraFinally, we are storing the top N hashtags per day in Cassandra. For that we&#8217;re using the row partitioning pattern to store a row per day and the top hashtags for each time bucket (20 seconds) 12345678910111213141516171819202122232425262728293031323334public class TopHashtagToCassandraBolt extends CassandraBaseBolt &#123; private static final Logger LOG = LoggerFactory.getLogger(TopHashtagToCassandraBolt.class); public TopHashtagToCassandraBolt(Properties properties) &#123; super(properties); &#125; @Override public void execute(Tuple tuple, BasicOutputCollector collector) &#123; List&lt;List&gt; rankings = (List) tuple.getValue(0); Map&lt;String, Long&gt; rankingMap = new HashMap&lt;&gt;(); for (List list : rankings) &#123; rankingMap.put((String) list.get(0), (Long) list.get(1)); &#125; DateFormat df = new SimpleDateFormat(\"yyyy-MM-dd\"); Statement statement = QueryBuilder.insertInto(\"top_hashtag_by_day\") .value(\"date\", df.format(new Date())) .value(\"bucket_time\", QueryBuilder.raw(\"dateof(now())\")) .value(\"ranking\", rankingMap); LOG.debug(statement.toString()); session.execute(statement); &#125; @Override public void declareOutputFields(OutputFieldsDeclarer declarer) &#123; &#125;&#125; Next stepsAlthough the hashtag counter may work, I will not say that is entirely correct and there are better ways to do it: You can take a look to this excellent resource: http://www.michael-noll.com/blog/2013/01/18/implementing-real-time-trending-topics-in-storm/ Using Storm Trident: In the next post I will show how to use the high level abstraction from Storm that allows to process a stream as a sequence of small batches of data (aka micro-batching) and fits better for the top hashtags example.","categories":[{"name":"Architecture","slug":"Architecture","permalink":"http://www.serrate.net/categories/Architecture/"}],"tags":[{"name":"BigData","slug":"BigData","permalink":"http://www.serrate.net/tags/BigData/"},{"name":"Storm","slug":"Storm","permalink":"http://www.serrate.net/tags/Storm/"}]},{"title":"Analysis of twitter streams with Kafka and Storm","slug":"analysis-of-twitter-streams-with-kafka-and-storm","date":"2016-01-05T01:24:07.000Z","updated":"2017-07-02T16:42:51.000Z","comments":true,"path":"2016/01/05/analysis-of-twitter-streams-with-kafka-and-storm/","link":"","permalink":"http://www.serrate.net/2016/01/05/analysis-of-twitter-streams-with-kafka-and-storm/","excerpt":"","text":"Following my last post, I will present a real-time processing sample with Kafka and Storm using the Twitter Streaming API. Overview The solution consists of the following: twitter-kafka-producer: A very basic producer that reads tweets from the Twitter Streaming API and stores them in Kafka. twitter-storm-topology: A Storm topology that reads tweets from Kafka and, after applying filtering and sanitization, process the messages in parallel for: Sentiment Analysis: Using a sentiment analysis algorithm to classify the tweet into a positive or negative feeling. Top Hashtags: Calculates the top 20 hashtags using a sliding window. Storm Topology The Storm topology consist of the following elements: Kafka Spout: The spout implementation to read messages from Kafka. Filtering: Filtering out all non-english language tweets. Sanitization: Text normalization in order to be processed properly by the sentiment analysis algorithm. Sentiment Analysis: The algorithm that analyses word by word the text of the tweet, giving a value between -1 to 1. Sentiment Analysis to Cassandra: Stores the tweets and its sentiment value in Cassandra. Hashtag Splitter: Splits the different hashtags appearing in a tweet. Hashtag Counter: Counts hashtag occurrences. Top Hashtag: Does a ranking of the top 20 hashtags given a sliding windows (using the Tick Tuple feature from Storm). Top Hashtag to Cassandra: Stores the top 20 hashtags in Cassandra. SummaryIn this post we have seen the benefits of using Apache Kafka &amp; Apache Storm to ingest and process streams of data, on next posts will look at the implementation details and will provide some analytical insight from the data stored in Cassandra. The sample can be found on Github: https://github.com/mserrate/twitter-streaming-app","categories":[{"name":"Architecture","slug":"Architecture","permalink":"http://www.serrate.net/categories/Architecture/"}],"tags":[{"name":"BigData","slug":"BigData","permalink":"http://www.serrate.net/tags/BigData/"},{"name":"Cassandra","slug":"Cassandra","permalink":"http://www.serrate.net/tags/Cassandra/"},{"name":"Kafka","slug":"Kafka","permalink":"http://www.serrate.net/tags/Kafka/"},{"name":"Storm","slug":"Storm","permalink":"http://www.serrate.net/tags/Storm/"}]},{"title":"Big Data: streams and lambdas","slug":"big-data-streams-and-lambdas","date":"2015-12-13T00:31:08.000Z","updated":"2017-07-02T16:16:13.000Z","comments":true,"path":"2015/12/13/big-data-streams-and-lambdas/","link":"","permalink":"http://www.serrate.net/2015/12/13/big-data-streams-and-lambdas/","excerpt":"","text":"I&#8217;ve been working for some years now in distributed systems and event-driven architectures, from the misunderstood SOA (or its refurbished version known as Microservices) to Event Sourcing. Some of the concepts presented in these systems related to events like immutability, perpetuity and versioning are valid as well for stream processing. Stream processing along with batch processing is sometimes referred as Big Data. Big DataWhen we think about Big Data what it first comes to our mind is Hadoop for batch processing. Although Hadoop has a big capacity to process indecent amounts of data, it also comes with a high latency response. Although this latency won&#8217;t be a problem for a lot of use cases, it may be a problem when we need to get real (or near-real) time feedback. That&#8217;s where the Lambda Architecture (by Nathan Marz) comes in by describing how to design a system where most of our data is processed by the batch layer but, while this process is running, we are able to process the streams coming into our system: Where we can say that: Current View = Query(Batch View) + Query(Stream View) Batch LayerThe batch processing layer computes arbitrary sets of data using the entire historical data. The obvious example of batch processing is Hadoop, or to be more precise, the distributed file system HDFS and a processing tool like MapReduce, Pigâ€¦ The result of this process will be stored in a database that should support batch writes (ElephantDB, HBase) but no random writes. That makes the database architecture extremely simple by removing features like online compactation or concurrency. Stream LayerThe stream processing layer computes data one by one giving immediate feedback. Depending on the number of events or the throughput needed we may use different technologies: Spark Streaming (although it&#8217;s micro-batch the latency may be sufficient for many use cases), Storm, Samza, Flink. The result of this process will be stored in a database that should support random writes, one option may be Cassandra. In following posts I will present concrete examples with docker images using some technologies that I&#8217;ve used like: Kafka, Storm, Cassandra and Druid.","categories":[{"name":"Architecture","slug":"Architecture","permalink":"http://www.serrate.net/categories/Architecture/"}],"tags":[{"name":"BigData","slug":"BigData","permalink":"http://www.serrate.net/tags/BigData/"}]},{"title":"Speaking at DotNetSpain conference","slug":"speaking-at-dotnetspain-conference","date":"2015-02-17T17:36:13.000Z","updated":"2017-07-02T16:06:26.000Z","comments":true,"path":"2015/02/17/speaking-at-dotnetspain-conference/","link":"","permalink":"http://www.serrate.net/2015/02/17/speaking-at-dotnetspain-conference/","excerpt":"","text":"Too long without posting&#8230; Anyway, a short post to remember that I will be speaking at DotNetSpain 2015 about Complex Event Processing, Immutability and Projections with EventStore. So, if you are interested come and say hi!","categories":[{"name":"Architecture","slug":"Architecture","permalink":"http://www.serrate.net/categories/Architecture/"}],"tags":[{"name":"CQRS","slug":"CQRS","permalink":"http://www.serrate.net/tags/CQRS/"},{"name":"EventStore","slug":"EventStore","permalink":"http://www.serrate.net/tags/EventStore/"}]},{"title":"Webcast about SOA, DDD & CQRS with NServiceBus","slug":"webcast-about-soa-ddd-cqrs-with-nservicebus","date":"2012-11-05T10:46:09.000Z","updated":"2017-07-02T16:04:09.000Z","comments":true,"path":"2012/11/05/webcast-about-soa-ddd-cqrs-with-nservicebus/","link":"","permalink":"http://www.serrate.net/2012/11/05/webcast-about-soa-ddd-cqrs-with-nservicebus/","excerpt":"","text":"I will be giving a Webcast tomorrow about SOA, DDD, CQRS with NServiceBus in Spanish. In this talk I will cover the DDD strategic design, bounded contexts and how to model domain logic through NServiceBus Sagas. You can see the details in the following link:","categories":[{"name":"Architecture","slug":"Architecture","permalink":"http://www.serrate.net/categories/Architecture/"},{"name":"NServiceBus","slug":"Architecture/NServiceBus","permalink":"http://www.serrate.net/categories/Architecture/NServiceBus/"}],"tags":[{"name":"DDD","slug":"DDD","permalink":"http://www.serrate.net/tags/DDD/"},{"name":"SOA","slug":"SOA","permalink":"http://www.serrate.net/tags/SOA/"},{"name":"CQRS","slug":"CQRS","permalink":"http://www.serrate.net/tags/CQRS/"},{"name":"NServiceBus","slug":"NServiceBus","permalink":"http://www.serrate.net/tags/NServiceBus/"}]},{"title":"NServiceBus: DRY with unobtrusive conventions","slug":"nservicebus-dry-with-unobtrusive-conventions","date":"2012-10-16T08:20:11.000Z","updated":"2017-07-02T16:01:45.000Z","comments":true,"path":"2012/10/16/nservicebus-dry-with-unobtrusive-conventions/","link":"","permalink":"http://www.serrate.net/2012/10/16/nservicebus-dry-with-unobtrusive-conventions/","excerpt":"","text":"Many times when working with NServiceBus in unobtrusive mode you may feel that you are repeating the same conventions over and over again on all the endpoints. The IWantToRunBeforeConfiguration interface is a great help in order to embrace the DRY principle. Just define your implementation in an assembly referenced by all the endpoints: 12345678910111213public class UnobtrusiveConventions : IWantToRunBeforeConfiguration&#123; public void Init() &#123; Configure.Instance .DefiningCommandsAs(t =&gt; t.Namespace != null &amp;&amp; t.Namespace.EndsWith(\"Commands\")) .DefiningEventsAs(t =&gt;; t.Namespace != null &amp;&amp; t.Namespace.EndsWith(\"Events\")) .DefiningMessagesAs(t =&gt; t.Namespace != null &amp;&amp; t.Namespace.EndsWith(\"Messages\")); &#125;&#125; and NServiceBus will pick this class automatically for each endpoint.","categories":[{"name":"Architecture","slug":"Architecture","permalink":"http://www.serrate.net/categories/Architecture/"},{"name":"NServiceBus","slug":"Architecture/NServiceBus","permalink":"http://www.serrate.net/categories/Architecture/NServiceBus/"}],"tags":[{"name":"SOA","slug":"SOA","permalink":"http://www.serrate.net/tags/SOA/"},{"name":"NServiceBus","slug":"NServiceBus","permalink":"http://www.serrate.net/tags/NServiceBus/"}]},{"title":"NServiceBus Training","slug":"nservicebus-training","date":"2012-09-11T21:28:47.000Z","updated":"2017-07-02T16:05:26.000Z","comments":true,"path":"2012/09/11/nservicebus-training/","link":"","permalink":"http://www.serrate.net/2012/09/11/nservicebus-training/","excerpt":"","text":"I will be giving the Udi Dahan&#8216;s Enterprise Development with NServiceBus 4-day course in Spain at: November 26. Barcelona December 10. Madrid More info at: http://udidahan.com/2012/10/12/training-for-this-winter/","categories":[{"name":"Architecture","slug":"Architecture","permalink":"http://www.serrate.net/categories/Architecture/"}],"tags":[{"name":"DDD","slug":"DDD","permalink":"http://www.serrate.net/tags/DDD/"},{"name":"SOA","slug":"SOA","permalink":"http://www.serrate.net/tags/SOA/"},{"name":"CQRS","slug":"CQRS","permalink":"http://www.serrate.net/tags/CQRS/"},{"name":"NServiceBus","slug":"NServiceBus","permalink":"http://www.serrate.net/tags/NServiceBus/"}]},{"title":"Cassandra on Azure CentOS VM","slug":"cassandra-on-azure-centos-vm","date":"2012-08-27T09:36:02.000Z","updated":"2017-07-02T15:57:36.000Z","comments":true,"path":"2012/08/27/cassandra-on-azure-centos-vm/","link":"","permalink":"http://www.serrate.net/2012/08/27/cassandra-on-azure-centos-vm/","excerpt":"Having some fun with Cassandra lately I wanted to figure out how to setup a working environment on the new Windows Azure VM roles, so I decided to give a try and install a Cassandra cluster on CentOS. Although itâ€™s on Ubuntu, the following article is a good guide that helped me to configure a Linux cluster: https://www.windowsazure.com/en-us/manage/linux/other-resources/how-to-run-cassandra-with-linux/ We create the 1st VM assigning a pem certificate in order to get access by ssh:","text":"Having some fun with Cassandra lately I wanted to figure out how to setup a working environment on the new Windows Azure VM roles, so I decided to give a try and install a Cassandra cluster on CentOS. Although itâ€™s on Ubuntu, the following article is a good guide that helped me to configure a Linux cluster: https://www.windowsazure.com/en-us/manage/linux/other-resources/how-to-run-cassandra-with-linux/ We create the 1st VM assigning a pem certificate in order to get access by ssh: and then we assign the DNS name: We will connect the remaining VMs to the 1st one on the following step: Cassandra installationIâ€™ve installed Cassandra from DataStax source because I found that packages and documentation are pretty good, but you can install from the Apache repository as well. Follow the next link for detailed instructions http://www.datastax.com/docs/1.1/install/install_rpm (just make sure to install Oracle JRE because CentOS comes with OpenJDK installed by default, and then use the alternatives command to make Oracle JRE the default one). You will need to open the 9160 TPC public port on all the VM&#8217;s in the cluster (so it will be load balanced). Also, you can install OpsCenter (a management and monitoring web UI tool) by following http://www.datastax.com/docs/opscenter/install/install_rhel. Then, you will need to open the OpsCenter port: It looks like this: And the Cassandra&#8217;s ring view: In next posts I will cover the use of Cassandra from .NET.","categories":[{"name":"NoSQL","slug":"NoSQL","permalink":"http://www.serrate.net/categories/NoSQL/"}],"tags":[{"name":"Azure","slug":"Azure","permalink":"http://www.serrate.net/tags/Azure/"},{"name":"BigData","slug":"BigData","permalink":"http://www.serrate.net/tags/BigData/"},{"name":"Cassandra","slug":"Cassandra","permalink":"http://www.serrate.net/tags/Cassandra/"},{"name":"NoSQL","slug":"NoSQL","permalink":"http://www.serrate.net/tags/NoSQL/"}]},{"title":"UI Composition for Services","slug":"ui-composition-for-services","date":"2012-08-22T07:22:49.000Z","updated":"2017-07-02T17:16:48.000Z","comments":true,"path":"2012/08/22/ui-composition-for-services/","link":"","permalink":"http://www.serrate.net/2012/08/22/ui-composition-for-services/","excerpt":"One of the most important concepts when applying either SOA or DDD is the definition of Services (or Bounded Contexts in the DDD lingo). Each of these services will be responsible for its own data and behavior and could also own the UI components of that service. Letâ€™s see an example with the typical Ecommerce domain:","text":"One of the most important concepts when applying either SOA or DDD is the definition of Services (or Bounded Contexts in the DDD lingo). Each of these services will be responsible for its own data and behavior and could also own the UI components of that service. Letâ€™s see an example with the typical Ecommerce domain: In this case we have two services, Sales &amp; Shipping, and each of them owns its UI components that will be rendered by the UI host. That&#8217;s the result: Composition with ASP.NET MVCIn this example, I&#8217;m using Razor Generator in order to let services to have their own MVC components in class libraries, which will be referenced by the central UI host: The use of Razor Generator is very straightforward: Install RazorGenerator from VS gallery On each class library, install RazorGenerator.Mvc from Nuget Create the folder structure following MVC conventions (Controllers, Views, etc.) Add a web.config to get intellisense for your views Change &#8220;Custom Tool&#8221; to RazorGenerator on your views in order to precompile it Then, decorate your actions as ChildActionOnly to behave like a widget: 12345678910111213141516171819202122232425262728293031323334353637383940414243public class FinishOrderController : Controller&#123; private readonly IBus bus; public FinishOrderController(IBus bus) &#123; this.bus = bus; &#125; [ChildActionOnly] public ActionResult SubmitOrCancel() &#123; ViewBag.OrderId = TheSession.OrderId; return PartialView(); &#125; [HttpPost] public ActionResult SubmitOrder() &#123; var cmd = new SubmitOrder() &#123; OrderId = TheSession.OrderId &#125;; this.bus.Send(cmd); return RedirectToAction(\"Processed\", \"Order\"); &#125; [HttpPost] public ActionResult CancelOrder() &#123; var cmd = new CancelOrder() &#123; OrderId = TheSession.OrderId &#125;; this.bus.Send(cmd); return RedirectToAction(\"Cancelled\", \"Order\"); &#125;&#125; Take a look at the sample on github and let me know!! https://github.com/mserrate/ui-composition/tree/master/UIComposition","categories":[{"name":"Architecture","slug":"Architecture","permalink":"http://www.serrate.net/categories/Architecture/"}],"tags":[{"name":"DDD","slug":"DDD","permalink":"http://www.serrate.net/tags/DDD/"},{"name":"SOA","slug":"SOA","permalink":"http://www.serrate.net/tags/SOA/"}]},{"title":"Welcome to my blog","slug":"welcome-to-my-blog","date":"2012-08-05T12:08:19.000Z","updated":"2017-07-02T15:10:50.000Z","comments":false,"path":"2012/08/05/welcome-to-my-blog/","link":"","permalink":"http://www.serrate.net/2012/08/05/welcome-to-my-blog/","excerpt":"","text":"I want to welcome you to my blog! Iâ€™d like to share with you my ramblings on various topics that I&#8217;m interested in: Distributed systems SOA DDD NoSQL Agile &#8230;and many more!! You can also visit my blog in spanish language at: http://www.serrate.es Hope to see you here!","categories":[{"name":"Blog","slug":"Blog","permalink":"http://www.serrate.net/categories/Blog/"}],"tags":[{"name":"Misc","slug":"Misc","permalink":"http://www.serrate.net/tags/Misc/"}]}]}